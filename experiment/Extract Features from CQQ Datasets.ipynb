{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "from time import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "np.warnings.filterwarnings('ignore')\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pysal\n",
    "import scipy as sc\n",
    "import scipy.stats as sc_stats\n",
    "from scipy.linalg import norm\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from scipy.stats import pearsonr, f_oneway, chi2_contingency, ks_2samp\n",
    "from scipy.stats import entropy, normaltest, mode, kurtosis, skew, pearsonr, moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_data_directory = '../experiment_data/'\n",
    "input_file = join(experiment_data_directory, 'all_datasets.tsv')\n",
    "output_file_name = join(experiment_data_directory, 'dataset_features.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = pd.read_table(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = [\n",
    "    'norm',\n",
    "    'lognorm',\n",
    "    'expon',\n",
    "    'powerlaw',\n",
    "    'uniform',\n",
    "    'chi2'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outlier_one_sided_MAD(points, thresh=3.5):\n",
    "    \"\"\"\n",
    "    From: https://stackoverflow.com/questions/22354094/pythonic-way-of-detecting-outliers-in-one-dimensional-observation-data\n",
    "    \n",
    "    Returns a boolean array with True if points are outliers and False \n",
    "    otherwise.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "        points : An numobservations by numdimensions array of observations\n",
    "        thresh : The modified z-score to use as a threshold. Observations with\n",
    "            a modified z-score (based on the median absolute deviation) greater\n",
    "            than this value will be classified as outliers.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        mask : A numobservations-length boolean array.\n",
    "\n",
    "    References:\n",
    "    ----------\n",
    "        Boris Iglewicz and David Hoaglin (1993), \"Volume 16: How to Detect and\n",
    "        Handle Outliers\", The ASQC Basic References in Quality Control:\n",
    "        Statistical Techniques, Edward F. Mykytka, Ph.D., Editor. \n",
    "    \"\"\"\n",
    "    if len(points.shape) == 1:\n",
    "        points = points[:,None]\n",
    "    median = np.median(points, axis=0)\n",
    "    diff = np.sum((points - median)**2, axis=-1)\n",
    "    diff = np.sqrt(diff)\n",
    "    med_abs_deviation = np.median(diff)\n",
    "\n",
    "    modified_z_score = 0.6745 * diff / med_abs_deviation\n",
    "\n",
    "    return modified_z_score > thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def madm(a):\n",
    "    a = np.ma.array(a).compressed() # should be faster to not use masked arrays.\n",
    "    med = np.median(a)\n",
    "    return np.median(np.abs(a- med))\n",
    "\n",
    "def get_shared_elements(v1, v2):\n",
    "    return [e for e in v1 if v1 in v2]\n",
    "\n",
    "def get_unique(li, preserve_order=False):\n",
    "    if preserve_order:\n",
    "        seen = set()\n",
    "        seen_add = seen.add\n",
    "        return [x for x in li if not (x in seen or seen_add(x))]\n",
    "    else:\n",
    "        return np.unique(li)\n",
    "\n",
    "def get_list_uniqueness(l):\n",
    "    if len(l):\n",
    "        return len(np.unique(l)) / len(l)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def detect_unique_list(l, THRESHOLD=0.95):\n",
    "    if len(l) and ((len(np.unique(l)) / len(l)) >= THRESHOLD):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def list_entropy(l):\n",
    "    return entropy(pd.Series(l).value_counts() / len(l))\n",
    "\n",
    "def gini(array):\n",
    "    \"\"\"Calculate the Gini coefficient of a numpy array.\"\"\"\n",
    "    # based on bottom eq: http://www.statsdirect.com/help/content/image/stat0206_wmf.gif\n",
    "    # from: http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm\n",
    "    array = array.flatten() #all values are treated equally, arrays must be 1d\n",
    "    if np.amin(array) < 0:\n",
    "        array -= np.amin(array) #values cannot be negative\n",
    "    array = np.add(array, 0.0000001) # values cannot be 0\n",
    "    array = np.sort(array) #values must be sorted\n",
    "    index = np.arange(1,array.shape[0]+1) #index per array element\n",
    "    n = array.shape[0]#number of array elements\n",
    "    return ((np.sum((2 * index - n  - 1) * array)) / (n * np.sum(array))) #Gini coefficient\n",
    "\n",
    "def calculate_overlap(a_data, b_data):\n",
    "    a_min, a_max = np.min(a_data), np.max(a_data)\n",
    "    a_range = a_max - a_min\n",
    "    b_min, b_max = np.min(b_data), np.max(b_data)\n",
    "    b_range = b_max - b_min\n",
    "    has_overlap = False\n",
    "    overlap_percent = 0\n",
    "    if (a_max >= b_min) and (b_min >= a_min):\n",
    "        has_overlap = True\n",
    "        overlap = (a_max - b_min)\n",
    "    if (b_max >= a_min) and (a_min >= b_min):\n",
    "        has_overlap = True\n",
    "        overlap = (b_max - a_min)\n",
    "    if has_overlap:\n",
    "        overlap_percent = max(overlap / a_range, overlap / b_range)\n",
    "    if ((b_max >= a_max) and (b_min <= a_min)) or ((a_max >= b_max) and (a_min <= b_min)):\n",
    "        has_overlap = True\n",
    "        overlap_percent = 1\n",
    "    return has_overlap, overlap_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_SQRT2 = np.sqrt(2)\n",
    "def hellinger(p, q):\n",
    "    return np.sqrt(np.sum((np.sqrt(p) - np.sqrt(q)) ** 2)) / _SQRT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = [\n",
    "    'norm',\n",
    "    'lognorm',\n",
    "    'expon',\n",
    "    'powerlaw',\n",
    "    'uniform',\n",
    "    'chi2'\n",
    "]\n",
    "\n",
    "def extract_q_features(v):\n",
    "    r = OrderedDict()\n",
    "    sample_mean = np.mean(v)\n",
    "    sample_median = np.median(v)\n",
    "    sample_var = np.var(v)\n",
    "    sample_min = np.min(v)\n",
    "    sample_max = np.max(v)\n",
    "    sample_std = np.std(v)\n",
    "    q1, q25, q75, q99 = np.percentile(v, [0.01, 0.25, 0.75, 0.99])\n",
    "    iqr = q75 - q25\n",
    "\n",
    "    r['mean'] = sample_mean\n",
    "    r['normalized_mean'] = sample_mean / sample_max\n",
    "    r['median'] = sample_median\n",
    "    r['normalized_median'] = sample_median / sample_max\n",
    "\n",
    "    r['var'] = sample_var\n",
    "    r['std'] = sample_std\n",
    "    r['coeff_var'] = (sample_mean / sample_var) if sample_var else None\n",
    "    r['min'] = sample_min\n",
    "    r['max'] = sample_max\n",
    "    r['range'] = r['max'] - r['min']\n",
    "    r['normalized_range'] = (r['max'] - r['min']) / sample_mean if sample_mean else None\n",
    "\n",
    "    r['entropy'] = None\n",
    "    try:\n",
    "        binned_values, bin_edges = get_binned_values(v)\n",
    "        r['entropy'] = entropy(binned_values)\n",
    "    except Exception as e: print(e, v)\n",
    "    r['gini'] = gini(v)\n",
    "    r['q25'] = q25\n",
    "    r['q75'] = q75\n",
    "    r['med_abs_dev'] = np.median(np.absolute(v - sample_median))\n",
    "    r['avg_abs_dev'] = np.mean(np.absolute(v - sample_mean))\n",
    "    r['quant_coeff_disp'] = (q75 - q25) / (q75 + q25)\n",
    "    r['coeff_var'] = sample_var / sample_mean\n",
    "    r['skewness'] = skew(v)\n",
    "    r['kurtosis'] = kurtosis(v)\n",
    "    r['moment_5'] = moment(v, moment=5)\n",
    "    r['moment_6'] = moment(v, moment=6)\n",
    "    r['moment_7'] = moment(v, moment=7)\n",
    "    r['moment_8'] = moment(v, moment=8)\n",
    "    r['moment_9'] = moment(v, moment=9)\n",
    "    r['moment_10'] = moment(v, moment=10)\n",
    "\n",
    "    # Outliers\n",
    "    outliers_15iqr = np.logical_or(v < (q25 - 1.5*iqr), v > (q75 + 1.5*iqr))\n",
    "    outliers_3iqr = np.logical_or(v < (q25 - 3*iqr), v > (q75 + 3*iqr))\n",
    "    outliers_1_99 = np.logical_or(v < q1, v > q99)\n",
    "    outliers_3std = np.logical_or(v < (sample_mean - 3*sample_std), v > (sample_mean + 3*sample_std))\n",
    "    r['percent_outliers_15iqr'] = np.sum(outliers_15iqr) / len(v)\n",
    "    r['percent_outliers_3iqr'] = np.sum(outliers_3iqr) / len(v)\n",
    "    r['percent_outliers_1_99'] = np.sum(outliers_1_99) / len(v)\n",
    "    r['percent_outliers_3std'] = np.sum(outliers_3std) / len(v)\n",
    "\n",
    "    r['has_outliers_15iqr'] = np.any(outliers_15iqr)\n",
    "    r['has_outliers_3iqr'] = np.any(outliers_3iqr)\n",
    "    r['has_outliers_1_99'] = np.any(outliers_1_99)\n",
    "    r['has_outliers_3std'] = np.any(outliers_3std)\n",
    "\n",
    "    #####\n",
    "    # Statistical Distribution\n",
    "    #####\n",
    "\n",
    "    # Normal\n",
    "    r['normality_statistic'] = None\n",
    "    r['normality_p'] = None\n",
    "    r['is_normal_5'] = None\n",
    "    r['is_normal_1'] = None\n",
    "    if len(v) >= 8:\n",
    "        normality_k2, normality_p = normaltest(v)\n",
    "        r['normality_statistic'] = normality_k2\n",
    "        r['normality_p'] = normality_p\n",
    "        r['is_normal_5'] = (normality_p < 0.05)\n",
    "        r['is_normal_1'] = (normality_p < 0.01)\n",
    "\n",
    "    # v = random.sample(list(v), min(len(v), num_samples))\n",
    "    for dist_name in distributions:\n",
    "        dist_function = getattr(sc_stats, dist_name)\n",
    "        statistic, p_value = sc_stats.kstest(v, dist_name, args=dist_function.fit(v))\n",
    "        r['{}_ks_statistic'.format(dist_name)] = statistic\n",
    "        r['{}_ks_p'.format(dist_name)] = p_value\n",
    "   \n",
    "    \n",
    "    sorted_v = np.sort(v)\n",
    "    sequence_incremental_division = np.divide(sorted_v[:-1], sorted_v[1:])\n",
    "    sequence_incremental_subtraction = np.diff(sorted_v)\n",
    "    \n",
    "    r['is_monotonic'] = np.all(sequence_incremental_subtraction <= 0) or np.all(sequence_incremental_subtraction >= 0)\n",
    "    r['sortedness'] = np.absolute(pearsonr(v, sorted_v)[0])  # or use inversions\n",
    "    r['is_sorted'] = np.array_equal(sorted_v, v)  # np.allclose(v, sorted_v)  # np.array_equal(sorted_v, v)\n",
    "\n",
    "    r['lin_space_sequence_coeff'] = np.std(sequence_incremental_subtraction) / np.mean(sequence_incremental_subtraction)\n",
    "    r['log_space_sequence_coeff'] = np.std(sequence_incremental_division) / np.mean(sequence_incremental_division)\n",
    "    r['is_lin_space'] = r['lin_space_sequence_coeff'] <= 0.001\n",
    "    r['is_log_space'] = r['log_space_sequence_coeff'] <= 0.001\n",
    "\n",
    "    unique_elements = get_unique(v)\n",
    "    r['num_unique_elements'] = unique_elements.size\n",
    "    r['unique_percent'] = (r['num_unique_elements'] / len(v))\n",
    "    r['is_unique'] = (r['num_unique_elements'] == len(v))\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_c_features(v):\n",
    "    r = OrderedDict()\n",
    "    unique_elements = get_unique(v)\n",
    "    r['num_unique_elements'] = unique_elements.size\n",
    "    r['unique_percent'] = (r['num_unique_elements'] / len(v))\n",
    "    r['is_unique'] = (r['num_unique_elements'] == len(v))\n",
    "    \n",
    "    r['entropy'] = list_entropy(v)\n",
    "\n",
    "    value_counts = pd.Series(v).value_counts()\n",
    "    r['percentage_of_mode'] = (value_counts.max() / len(v))\n",
    "    r['same_num_per_group'] = (len(set(value_counts)) == len(value_counts))\n",
    "    r['min_per_group'] = np.min(value_counts)\n",
    "    r['max_per_group'] = np.max(value_counts)\n",
    "    r['mean_per_group'] = np.mean(value_counts)\n",
    "    r['std_per_group'] = np.std(value_counts)\n",
    "    \n",
    "    sorted_v = np.sort(v)\n",
    "    r['is_sorted'] = np.array_equal(sorted_v, v)\n",
    "    return r\n",
    "    \n",
    "def extract_qq_features(q1_values, q2_values):\n",
    "    r = OrderedDict()\n",
    "    df = pd.DataFrame({ 'W': q1_values, 'Z': q2_values })\n",
    "    \n",
    "    correlation_value, correlation_p = pearsonr(q1_values, q2_values)\n",
    "    ks_statistic, ks_p = ks_2samp(q1_values, q2_values)\n",
    "    has_overlap, overlap_percent = calculate_overlap(q1_values, q2_values)\n",
    "        \n",
    "    r['correlation_value'] = correlation_value\n",
    "    r['correlation_abs_value'] = np.abs(correlation_value)\n",
    "    r['correlation_p'] = correlation_p\n",
    "    r['correlation_significant_005'] = (correlation_p < 0.05)\n",
    "\n",
    "    r['ks_statistic'] = ks_statistic\n",
    "    r['ks_p'] = ks_p\n",
    "    r['ks_significant_005'] = (ks_p < 0.05)\n",
    "\n",
    "    r['has_overlap'] = has_overlap\n",
    "    r['overlap_percent'] = overlap_percent\n",
    "    \n",
    "    q_points = np.array(list(zip(q1_values, q2_values)))\n",
    "    outliers = detect_outlier_one_sided_MAD(q_points)\n",
    "    num_outliers = len([ o for o in outliers if o ])\n",
    "    percent_outliers = num_outliers / df.shape[0]\n",
    "    r['has_outliers_one-sided-MAD'] = any(outliers)\n",
    "    r['num_outliers_one-sided-MAD'] = num_outliers\n",
    "    r['percent_outliers_one-sided-MAD'] = percent_outliers\n",
    "    \n",
    "    r['entropy'] = None\n",
    "    r['morans_i_value'] = None\n",
    "    r['morans_i_p'] = None\n",
    "    try:\n",
    "        _, W_bin_edges = get_binned_values(df['W'])\n",
    "        _, Z_bin_edges = get_binned_values(df['Z'])\n",
    "        count_by_bins = df.groupby([pd.cut(df['W'], W_bin_edges, include_lowest=True), pd.cut(df['Z'], Z_bin_edges, include_lowest=True)]).count()['W'].fillna(0)\n",
    "        count_by_bins_array = count_by_bins.values\n",
    "        r['entropy'] = entropy(count_by_bins_array)\n",
    "\n",
    "        count_by_bins_matrix = count_by_bins.unstack().as_matrix()\n",
    "        w = pysal.lat2W(count_by_bins_matrix.shape[0], count_by_bins_matrix.shape[1])\n",
    "        mi = pysal.Moran(count_by_bins_matrix, w)\n",
    "        r['morans_i_value'] = mi.I\n",
    "        r['morans_i_p'] = mi.p_norm\n",
    "    except Exception as e: \n",
    "        print('QQ entropy and moran:', e, v)\n",
    "\n",
    "    return r\n",
    "\n",
    "def extract_cq_features(c_values, q_values):\n",
    "    r = OrderedDict()\n",
    "    df = pd.DataFrame({ 'C': c_values, 'Q': q_values })\n",
    "    \n",
    "    unique_c_field_values = get_unique(c_values)\n",
    "    group_values = [ df[df['C'] == v]['Q'] for v in unique_c_field_values ]\n",
    "    anova_result = f_oneway(*group_values)  \n",
    "\n",
    "    r['one_way_anova_statistic'] = anova_result.statistic\n",
    "    r['one_way_anova_p'] = anova_result.pvalue\n",
    "    r['one_way_anova_significant_005'] = (anova_result.pvalue < 0.05)\n",
    "\n",
    "    r['entropy'] = None\n",
    "    r['morans_i_value'] = None\n",
    "    r['morans_i_p'] = None\n",
    "    try:\n",
    "        _, bin_edges = get_binned_values(q_values, normalize=False)\n",
    "        count_by_bins = df.groupby(['C', pd.cut(q_values, bin_edges, include_lowest=True)]).count()['Q'].fillna(0)\n",
    "        count_by_bins_array = count_by_bins.values   \n",
    "        r['entropy'] = entropy(count_by_bins_array)\n",
    "\n",
    "        count_by_bins_matrix = count_by_bins.unstack().as_matrix()\n",
    "        w = pysal.lat2W(count_by_bins_matrix.shape[0], count_by_bins_matrix.shape[1])\n",
    "        mi = pysal.Moran(count_by_bins_matrix, w)\n",
    "        r['morans_i_value'] = mi.I\n",
    "        r['morans_i_p'] = mi.p_norm\n",
    "    except Exception as e:\n",
    "        print('CQ entropy and moran:', e, v)\n",
    "    return r\n",
    "\n",
    "# Global clusteredness\n",
    "# 1) Find distances from mean point\n",
    "# 2) Remove mean from distances\n",
    "# 3) Divide demeaned distances by standard deviation\n",
    "def get_global_clusteredness(v1, v2):\n",
    "    combined_v = np.array(list(zip(v1, v2)))\n",
    "    mean_point = np.mean(combined_v, axis=0)\n",
    "    distances = [ np.linalg.norm([e, mean_point]) for e in combined_v ]\n",
    "    z_score_distances = np.abs((distances - np.mean(distances)) / np.std(distances))\n",
    "\n",
    "    z_score_distances = [ x for x in z_score_distances if not pd.isnull(x) ] \n",
    "    clusteredness = np.sum(z_score_distances)\n",
    "    return clusteredness / v1.size\n",
    "\n",
    "def normalize(v):\n",
    "    return (v - np.min(v)) / (np.max(v) - np.min(v))\n",
    "\n",
    "def get_binned_values(v, bins=20, normalize=True):\n",
    "    hist_values, bin_edges = np.histogram(v, bins=bins, density=False)\n",
    "    if normalize:\n",
    "        hist_values = hist_values / hist_values.sum()\n",
    "    return hist_values, bin_edges\n",
    "\n",
    "def extract_cqq_features(c_values, w_values, z_values):\n",
    "    r = OrderedDict()\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'C': c_values,\n",
    "        'W': w_values,\n",
    "        'Z': z_values\n",
    "    })\n",
    "    \n",
    "    df.dropna(how='any', inplace=True)\n",
    "    \n",
    "    normalized_w_values = normalize(w_values)\n",
    "    normalized_z_values = normalize(z_values)\n",
    "    df['W'] = normalized_w_values\n",
    "    df['Z'] = normalized_z_values\n",
    "    \n",
    "    group_means = []\n",
    "    clusteredness_by_group = []\n",
    "    hellingers_distances = []\n",
    "    for name, group in df.groupby('C'):\n",
    "        means = np.mean(group)\n",
    "        w_values, z_values = group['W'], group['Z']\n",
    "        \n",
    "        # Probability distributions\n",
    "        try:\n",
    "            w_normalized_values, _= get_binned_values(w_values)\n",
    "            z_normalized_values, _ = get_binned_values(z_values)\n",
    "\n",
    "            hellinger_d = hellinger(w_normalized_values, z_normalized_values)\n",
    "            hellingers_distances.append(hellinger_d)\n",
    "\n",
    "            clusteredness_by_group.append(get_global_clusteredness(w_values, z_values))\n",
    "            group_means.append([np.mean(w_values), np.mean(z_values)])\n",
    "        except Exception as e:\n",
    "            print('Error calculating hellingers', e)\n",
    "            continue\n",
    "    \n",
    "    group_means_df = pd.DataFrame(group_means, columns=['W', 'Z'])\n",
    "\n",
    "    global_clusteredness = 1 - get_global_clusteredness(normalized_w_values, normalized_z_values)\n",
    "    group_mean_clusteredness = 1 - get_global_clusteredness(group_means_df['W'], group_means_df['Z'])\n",
    "    mean_of_group_clusteredness = 1 - np.mean(clusteredness_by_group)\n",
    "    \n",
    "#     sns.scatterplot(x='W', y='Z', hue='C', data=df)\n",
    "#     plt.show()\n",
    "#     print('Global', global_clusteredness)\n",
    "#     print('Group mean', group_mean_clusteredness)\n",
    "#     print('Mean of group', mean_of_group_clusteredness)\n",
    "\n",
    "    r['global_clusteredness'] = global_clusteredness\n",
    "    r['group_mean_clusteredness'] = group_mean_clusteredness\n",
    "    r['mean_of_group_clusteredness'] = mean_of_group_clusteredness\n",
    "    \n",
    "    \n",
    "    r['entropy'] = None\n",
    "    try:\n",
    "        _, W_bin_edges = get_binned_values(df['W'])\n",
    "        _, Z_bin_edges = get_binned_values(df['Z'])\n",
    "        count_by_bins = df.groupby([pd.cut(df['W'], W_bin_edges, include_lowest=True), pd.cut(df['Z'], Z_bin_edges, include_lowest=True)]).count()['W'].fillna(0)\n",
    "        count_by_bins_array = count_by_bins.values\n",
    "        r['entropy'] = entropy(count_by_bins_array)\n",
    "    except Exception as e: \n",
    "        print('CQQ entropy and moran:', e, v)\n",
    "\n",
    "    r['mean_hellingers'] = np.mean(hellingers_distances)\n",
    "    r['min_hellingers'] = np.min(hellingers_distances)\n",
    "    r['max_hellingers'] = np.max(hellingers_distances)\n",
    "    r['std_hellingers'] = np.std(hellingers_distances)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_header = True\n",
    "num_datasets = all_datasets.shape[0]\n",
    "\n",
    "batch_size = 10\n",
    "batch_features = []\n",
    "\n",
    "start_time = time()\n",
    "for i, row in all_datasets.iterrows():\n",
    "    i = i + 1\n",
    "    features = OrderedDict()\n",
    "    corpus = row['corpus']\n",
    "    dataset_id_with_combination_number = row['dataset_id']\n",
    "    dataset_id = dataset_id_with_combination_number.rsplit('__')[0]\n",
    "    df = pd.read_json(row['data'])\n",
    "    \n",
    "    C = np.ma.array(df['C']).compressed()\n",
    "    W = np.ma.array(df['W']).compressed()\n",
    "    Z = np.ma.array(df['Z']).compressed()\n",
    "    \n",
    "    features['corpus'] = corpus\n",
    "    features['dataset_id'] = dataset_id\n",
    "    features['dataset_id_with_combination_number'] = dataset_id_with_combination_number\n",
    "    \n",
    "    features['n_rows'] = df.shape[0]\n",
    "    \n",
    "    try:\n",
    "        r = extract_c_features(C)\n",
    "        for k, v in r.items(): features['c_{}'.format(k)] = v\n",
    "\n",
    "        r = extract_q_features(W)\n",
    "        for k, v in r.items(): features['q1_{}'.format(k)] = v\n",
    "\n",
    "        r = extract_q_features(Z)\n",
    "        for k, v in r.items(): features['q2_{}'.format(k)] = v\n",
    "\n",
    "        r = extract_cq_features(C, W)\n",
    "        for k, v in r.items(): features['cq1_{}'.format(k)] = v\n",
    "\n",
    "        r = extract_cq_features(C, Z)\n",
    "        for k, v in r.items(): features['cq2_{}'.format(k)] = v\n",
    "\n",
    "        r = extract_qq_features(W, Z)\n",
    "        for k, v in r.items(): features['qq_{}'.format(k)] = v\n",
    "\n",
    "        r = extract_cqq_features(C, W, Z)\n",
    "        for k, v in r.items(): features['cqq_{}'.format(k)] = v\n",
    "            \n",
    "        batch_features.append(features)\n",
    "    except Exception as e:\n",
    "        print('Uncaught high-level error:', e)\n",
    "        continue\n",
    "    \n",
    "    if i % batch_size == 0:\n",
    "        print('({}/{})'.format(i, num_datasets))\n",
    "        elapsed_time = time() - start_time\n",
    "        time_per_dataset = elapsed_time / i\n",
    "        time_to_finish = (num_datasets * time_per_dataset) - elapsed_time\n",
    "        print('Time to completion: {}'.format(datetime.timedelta(seconds=time_to_finish)))\n",
    "        \n",
    "        batch_features_df = pd.DataFrame(batch_features)\n",
    "        batch_features_df.to_csv(output_file_name, sep='\\t', mode='a', index=False, header=write_header)\n",
    "        print(batch_features_df)\n",
    "        batch_features = []\n",
    "        write_header=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'W': np.random.normal(size=1000),\n",
    "    'Z': np.random.normal(size=1000)\n",
    "})\n",
    "\n",
    "sns.scatterplot(x='W', y='Z', data=df)\n",
    "\n",
    "_, W_bin_edges = get_binned_values(df['W'])\n",
    "_, Z_bin_edges = get_binned_values(df['Z'])\n",
    "grouped_by_bins = df.groupby([pd.cut(df['W'], W_bin_edges, include_lowest=True), pd.cut(df['Z'], Z_bin_edges, include_lowest=True)])\n",
    "count_by_bins_matrix = grouped_by_bins.count()['W'].fillna(0).unstack().as_matrix()\n",
    "w = pysal.lat2W(count_by_bins_matrix.shape[0], count_by_bins_matrix.shape[1])\n",
    "mi = pysal.Moran(count_by_bins_matrix, w)\n",
    "print(mi.I, mi.EI, mi.p_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'W': np.random.uniform(size=1000),\n",
    "    'Z': np.random.uniform(size=1000)\n",
    "})\n",
    "\n",
    "sns.scatterplot(x='W', y='Z', data=df)\n",
    "\n",
    "_, W_bin_edges = get_binned_values(df['W'])\n",
    "_, Z_bin_edges = get_binned_values(df['Z'])\n",
    "grouped_by_bins = df.groupby([pd.cut(df['W'], W_bin_edges, include_lowest=True), pd.cut(df['Z'], Z_bin_edges, include_lowest=True)])\n",
    "count_by_bins_matrix = grouped_by_bins.count()['W'].fillna(0).unstack().as_matrix()\n",
    "w = pysal.lat2W(count_by_bins_matrix.shape[0], count_by_bins_matrix.shape[1])\n",
    "mi = pysal.Moran(count_by_bins_matrix, w)\n",
    "print(mi.I, mi.EI, mi.p_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'C': [ 'a', 'b', 'b', 'b', 'b', 'c'],\n",
    "    'Q': [ 1, 2, 2, 2, 2, 3]\n",
    "})\n",
    "\n",
    "q1_values = df['Q']\n",
    "_, bin_edges = get_binned_values(q1_values, normalize=False)\n",
    "count_by_bins = df.groupby(['C', pd.cut(q1_values, bin_edges, include_lowest=True)]).count()['Q'].fillna(0)\n",
    "count_by_bins_array = count_by_bins.values\n",
    "count_by_bins_matrix = count_by_bins.unstack().as_matrix()\n",
    "print(count_by_bins.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
