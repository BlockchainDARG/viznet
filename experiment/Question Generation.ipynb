{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.warnings.filterwarnings('ignore')\n",
    "import traceback\n",
    "import shutil\n",
    "\n",
    "import random\n",
    "from slugify import slugify\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path: sys.path.append(nb_dir)\n",
    "\n",
    "\n",
    "import altair as alt\n",
    "from altair import datum\n",
    "from IPython.display import display\n",
    "\n",
    "# for the notebook only (not for JupyterLab) run this command once per session\n",
    "alt.renderers.enable('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_type_to_dtype = {\n",
    "    'c': str,\n",
    "    'q': np.float64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '../experiment_data'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_specs = [{\"description\": {\"id\": 0}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v3.0.0-rc5.json\", \"mark\": \"point\", \"encoding\": {\"x\": {\"field\": \"Q1\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Q2\", \"type\": \"quantitative\"}, \"color\": {\"field\": \"name\", \"type\": \"nominal\"}}}, {\"description\": {\"id\": 1}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v3.0.0-rc5.json\", \"mark\": \"point\", \"encoding\": {\"x\": {\"field\": \"Q1\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Q2\", \"type\": \"quantitative\"}, \"row\": {\"field\": \"name\", \"type\": \"nominal\"}}}, {\"description\": {\"id\": 2}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v3.0.0-rc5.json\", \"mark\": \"point\", \"encoding\": {\"x\": {\"field\": \"Q1\", \"type\": \"quantitative\"}, \"color\": {\"field\": \"Q2\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"name\", \"type\": \"nominal\"}}}, {\"description\": {\"id\": 3}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v3.0.0-rc5.json\", \"mark\": \"point\", \"encoding\": {\"x\": {\"field\": \"Q1\", \"type\": \"quantitative\"}, \"size\": {\"field\": \"Q2\", \"type\": \"quantitative\", \"scale\": {\"range\": [1, 400]}}, \"y\": {\"field\": \"name\", \"type\": \"nominal\"}}}, {\"description\": {\"id\": 4}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v3.0.0-rc5.json\", \"mark\": \"point\", \"encoding\": {\"color\": {\"field\": \"Q1\", \"type\": \"quantitative\"}, \"x\": {\"field\": \"Q2\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"name\", \"type\": \"nominal\"}}}, {\"description\": {\"id\": 5}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v3.0.0-rc5.json\", \"mark\": \"point\", \"encoding\": {\"size\": {\"field\": \"Q1\", \"type\": \"quantitative\", \"scale\": {\"range\": [1, 400]}}, \"x\": {\"field\": \"Q2\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"name\", \"type\": \"nominal\"}}}, {\"description\": {\"id\": 6}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v3.0.0-rc5.json\", \"mark\": \"point\", \"encoding\": {\"y\": {\"field\": \"Q1\", \"type\": \"quantitative\"}, \"x\": {\"field\": \"Q2\", \"type\": \"quantitative\"}, \"color\": {\"field\": \"name\", \"type\": \"nominal\"}}}, {\"description\": {\"id\": 7}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v3.0.0-rc5.json\", \"mark\": \"point\", \"encoding\": {\"y\": {\"field\": \"Q1\", \"type\": \"quantitative\"}, \"x\": {\"field\": \"Q2\", \"type\": \"quantitative\"}, \"row\": {\"field\": \"name\", \"type\": \"nominal\"}}}, {\"description\": {\"id\": 8}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v3.0.0-rc5.json\", \"mark\": \"point\", \"encoding\": {\"y\": {\"field\": \"Q1\", \"type\": \"quantitative\"}, \"color\": {\"field\": \"Q2\", \"type\": \"quantitative\"}, \"x\": {\"field\": \"name\", \"type\": \"nominal\"}}}, {\"description\": {\"id\": 9}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v3.0.0-rc5.json\", \"mark\": \"point\", \"encoding\": {\"y\": {\"field\": \"Q1\", \"type\": \"quantitative\"}, \"size\": {\"field\": \"Q2\", \"type\": \"quantitative\", \"scale\": {\"range\": [1, 400]}}, \"x\": {\"field\": \"name\", \"type\": \"nominal\"}}}, {\"description\": {\"id\": 10}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v3.0.0-rc5.json\", \"mark\": \"point\", \"encoding\": {\"color\": {\"field\": \"Q1\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Q2\", \"type\": \"quantitative\"}, \"x\": {\"field\": \"name\", \"type\": \"nominal\"}}}, {\"description\": {\"id\": 11}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v3.0.0-rc5.json\", \"mark\": \"point\", \"encoding\": {\"size\": {\"field\": \"Q1\", \"type\": \"quantitative\", \"scale\": {\"range\": [1, 400]}}, \"y\": {\"field\": \"Q2\", \"type\": \"quantitative\"}, \"x\": {\"field\": \"name\", \"type\": \"nominal\"}}}]\n",
    "json.dump(raw_specs, open(join(output_dir, 'specs.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "encoding_type = 'x', 'y', 'color', 'size'\n",
    "field_abbreviation = 'c', 'q1', 'q2'\n",
    "abbreviation_to_field_name = {  c: c_field_name, q1: q1_field_name, q2: q2_field_name}\n",
    "'''\n",
    "def get_encoding_expansion(encoding_type, field_abbreviation, abbreviation_to_field_name):\n",
    "    result = { \n",
    "        'field': abbreviation_to_field_name[field_abbreviation],\n",
    "        'type': 'quantitative' if field_abbreviation in [ 'q1', 'q2' ] else 'nominal'\n",
    "    }\n",
    "    if encoding_type == 'size':\n",
    "        result['scale'] = { 'range': [1, 400] }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question_find_value(df, abbreviation_to_field_name):\n",
    "    c_name = abbreviation_to_field_name['c']\n",
    "    q1_name, q2_name = abbreviation_to_field_name['q1'], abbreviation_to_field_name['q2']\n",
    "    \n",
    "    # Filter out cases where both Q points are null\n",
    "    both_q_null = df[[ q1_name, q2_name ]].isnull().any(axis=1)\n",
    "    df = df[~both_q_null]\n",
    "\n",
    "    q1_values = df[q1_name]\n",
    "    q1_max = np.nanmax(q1_values)\n",
    "    q1_min = np.nanmin(q1_values)\n",
    "    q1_range = np.abs(q1_max - q1_min)\n",
    "\n",
    "    point = df.sample(1)\n",
    "    correct_value = point[q1_name].item()\n",
    "    \n",
    "    mid_point = (q1_max + q1_min) / 2\n",
    "    mid_range = (q1_max - q1_min) / 2\n",
    "\n",
    "    if correct_value > mid_point:\n",
    "        incorrect_value = correct_value - (mid_range).item()\n",
    "    else: \n",
    "        incorrect_value = correct_value + (mid_range).item()\n",
    "    \n",
    "    if correct_value == incorrect_value:\n",
    "        return None\n",
    "    \n",
    "    # incorrect_value = (correct_value - q1_range / 2).item() if (random.random() < 0.5) else (correct_value + q1_range).item()\n",
    "    question_text = {\n",
    "        'text': 'What is the <b>{}</b> value of the data point A?'.format(q1_name),\n",
    "        'structure': [\n",
    "            { 'type': 'text', 'value': 'What is the' },\n",
    "            { 'type': 'field', 'value': q1_name },\n",
    "            { 'type': 'text', 'value': 'of the data point A?' }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Randomize options\n",
    "    options = [ correct_value, incorrect_value ]\n",
    "    correct = 0\n",
    "    if random.random() < 0.5:\n",
    "        options = [ incorrect_value, correct_value ]\n",
    "        correct = 1\n",
    "    \n",
    "    annotation = {\n",
    "        'annotated': 'A'\n",
    "    }\n",
    "    annotation[c_name] = point[c_name].item()\n",
    "    annotation[q1_name] = point[q1_name].item()\n",
    "    annotation[q2_name] = point[q2_name].item()\n",
    "    annotations = [ annotation ]\n",
    "\n",
    "    question = {\n",
    "        'q_field': q1_name,\n",
    "        'q1': q1_name,\n",
    "        'q2': q2_name,\n",
    "        'c': c_name,\n",
    "        'question': question_text,\n",
    "        'options': options,\n",
    "        'correct': correct,\n",
    "        'task': 'find_value',\n",
    "        'annotated': annotations\n",
    "    }\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question_compare_values(df, abbreviation_to_field_name):\n",
    "    c_name, q1_name, q2_name = abbreviation_to_field_name['c'], abbreviation_to_field_name['q1'], abbreviation_to_field_name['q2']\n",
    "\n",
    "    # Randomly decide if A and B are from same or different group\n",
    "    sample_same_group = random.choice([ True, False ])\n",
    "    groups = list(df[c_name].unique())\n",
    "    if sample_same_group: group_a = group_b = random.sample(groups, 1)[0]\n",
    "    else: group_a, group_b = random.sample(groups, 2)\n",
    "    \n",
    "    # Get Range\n",
    "    q1_values = df[q1_name]\n",
    "    q1_max = np.nanmax(q1_values)\n",
    "    q1_min = np.nanmin(q1_values)\n",
    "    q1_range = np.abs(q1_max - q1_min)\n",
    "    q1_mid_range = q1_range / 2\n",
    "    q1_mid_point = (q1_max + q1_min) / 2\n",
    "\n",
    "    # Select Point A\n",
    "    point_a = df[df[c_name] == group_a].sample(1)\n",
    "    point_a_q1 = point_a[q1_name].item()\n",
    "\n",
    "    # Select Point B\n",
    "    if point_a_q1 < q1_mid_point:\n",
    "        ideal_point_b_q1 = point_a_q1 + q1_mid_range\n",
    "    else:\n",
    "        ideal_point_b_q1 = point_a_q1 - q1_mid_range\n",
    "    \n",
    "    identical_points = (df[c_name] == point_a[c_name].item()) & (df[q1_name] == point_a[q1_name].item()) & (df[q2_name] == point_a[q2_name].item())\n",
    "    df_excluding_point_a = df[~identical_points]\n",
    "    df_subset = df_excluding_point_a[df_excluding_point_a[c_name] == group_b]\n",
    "    \n",
    "    point_b_candidate_distances = np.abs(df_subset[q1_name] - ideal_point_b_q1)\n",
    "    point_b = pd.DataFrame(df.iloc[point_b_candidate_distances.idxmin(), :]).transpose()\n",
    "    point_b_q1 = point_b[q1_name].item()\n",
    "    \n",
    "    more_or_less = random.choice(['higher', 'lower'])\n",
    "    if more_or_less == 'more': correct = 0 if (point_a_q1 > point_b_q1) else 1\n",
    "    else: correct = 0 if (point_a_q1 < point_b_q1) else 1\n",
    "            \n",
    "    question_text = {\n",
    "        'text': 'Which data point has a <u>{}</u> value of <b>{}</b>?'.format(more_or_less, q1_name),\n",
    "        'structure': [\n",
    "            { 'type': 'text', 'value': 'Which data point has' },\n",
    "            { 'type': 'operator', 'value': more_or_less },\n",
    "            { 'type': 'field', 'value': q1_name }\n",
    "        ]  \n",
    "    }\n",
    "    \n",
    "    annotations = []\n",
    "    \n",
    "    if (point_a[q1_name].item() == point_b[q1_name].item()) and \\\n",
    "        (point_a[q2_name].item() == point_b[q2_name].item()) and \\\n",
    "        (point_a[c_name].item() == point_b[c_name].item()):\n",
    "        print('Point A and Point B are the same')\n",
    "        print(point_a, point_b)\n",
    "        return None\n",
    "    \n",
    "    for point, point_name in [ (point_a, 'A'), (point_b, 'B') ]:\n",
    "        annotation = { 'annotated': point_name }\n",
    "        annotation[c_name] = point[c_name].item() # annotation_c_name\n",
    "        annotation[q1_name] = point[q1_name].item()\n",
    "        annotation[q2_name] = point[q2_name].item()\n",
    "        \n",
    "        if any(pd.isnull([ point[c_name].item(), point[q1_name].item(), point[q2_name].item() ])):\n",
    "            print(point[c_name].item(), point[q1_name].item(), point[q2_name].item())\n",
    "            return None\n",
    "            \n",
    "        annotations.append(annotation)\n",
    "\n",
    "    question = {\n",
    "        'q_field': q1_name,\n",
    "        'q1': q1_name,\n",
    "        'q2': q2_name,\n",
    "        'c': c_name,\n",
    "        'question': question_text,\n",
    "        'options': [ 'A', 'B' ],\n",
    "        'correct': correct,\n",
    "        'task': 'compare_values',\n",
    "        'annotated': annotations,\n",
    "        'metadata': {\n",
    "            'same_group': sample_same_group\n",
    "        }\n",
    "    }\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question_find_maximum(df, abbreviation_to_field_name):\n",
    "    c_name, q1_name, q2_name = abbreviation_to_field_name['c'], abbreviation_to_field_name['q1'], abbreviation_to_field_name['q2']\n",
    "\n",
    "    # Find category of point containing highest q1 (M)\n",
    "    q1_values = df[q1_name]\n",
    "    row_with_highest_q1_value = df.iloc[[q1_values.idxmax()]]\n",
    "    highest_q1_value = row_with_highest_q1_value[q1_name].item()\n",
    "    category_with_highest_q1_value = row_with_highest_q1_value[c_name].item()\n",
    "    \n",
    "    # Find category with highest q1 value closest to M - range(q1) / 2\n",
    "    range_q1 = np.abs(np.nanmax(q1_values) - np.nanmin(q1_values))\n",
    "    target_second_category_value = np.nanmax(q1_values) - (range_q1 / 2)\n",
    "    df_max_by_category = df[df[c_name] != category_with_highest_q1_value].groupby([ c_name ]).max()\n",
    "    df_sorted_distance_from_target = df_max_by_category.iloc[(df_max_by_category[q1_name] - target_second_category_value).abs().argsort()[:1]]\n",
    "    category_with_closest_to_target_q1_value = df_sorted_distance_from_target.index.values[0]\n",
    "    \n",
    "    # Populate options\n",
    "    random_choice = random.choice([0, 1])  # Which option is correct\n",
    "    correct = random_choice\n",
    "    reverse = -1 if random_choice else 1\n",
    "    options = [ category_with_highest_q1_value, category_with_closest_to_target_q1_value ][::reverse]\n",
    "\n",
    "    # Generate question test\n",
    "    question_text = {\n",
    "        'text': 'Which value of <b>{}</b> contains the data point with the highest value of <b>{}</b>?'.format(c_name, q1_name),\n",
    "        'structure': [\n",
    "            { 'type': 'text', 'value': 'Which' },\n",
    "            { 'type': 'field', 'value': c_name },\n",
    "            { 'type': 'text', 'value': 'has the data point with the highest' },\n",
    "            { 'type': 'field', 'value': q1_name }\n",
    "        ]  \n",
    "    }\n",
    "\n",
    "    # Return question\n",
    "    question = {\n",
    "        'q_field': q1_name,\n",
    "        'q1': q1_name,\n",
    "        'q2': q2_name,\n",
    "        'c': c_name,\n",
    "        'question': question_text,\n",
    "        'options': options,\n",
    "        'correct': correct,\n",
    "        'task': 'compare_values',\n",
    "        'annotated': []\n",
    "    }\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question_compare_averages(df, abbreviation_to_field_name):\n",
    "    c_name, q1_name, q2_name = abbreviation_to_field_name['c'], abbreviation_to_field_name['q1'], abbreviation_to_field_name['q2']\n",
    "\n",
    "    # Find category averages and target_difference\n",
    "    q1_values = df[q1_name]\n",
    "    range_q1 = np.abs(np.nanmax(q1_values) - np.nanmin(q1_values))\n",
    "    df_average_by_category = df.groupby([ c_name ]).mean()\n",
    "    target_difference_of_averages = 0.3 * range_q1\n",
    "\n",
    "    # Calculate difference of means of each group\n",
    "    differences_by_pair = []\n",
    "    groups = list(df[c_name].unique())\n",
    "    for ( group_1, group_2 ) in combinations(groups, 2):\n",
    "        group_1_q1 = df_average_by_category.loc[group_1][q1_name]\n",
    "        group_2_q1 = df_average_by_category.loc[group_2][q1_name]\n",
    "        difference = np.abs(group_1_q1 - group_2_q1)\n",
    "    \n",
    "        differences_by_pair.append({\n",
    "            'group_1': group_1,\n",
    "            'group_2': group_2,\n",
    "            'difference': difference\n",
    "        })\n",
    "        \n",
    "    # Get closest match\n",
    "    df_differences_by_pair = pd.DataFrame(differences_by_pair)\n",
    "    closest_matching_pair = df_differences_by_pair.iloc[(df_differences_by_pair['difference'] - target_difference_of_averages).abs().argsort()[:1]]\n",
    "    \n",
    "    # Get means of final groups\n",
    "    group_1, group_2 = closest_matching_pair['group_1'], closest_matching_pair['group_2']  # Accessor indices\n",
    "    mean_group_1, mean_group_2 = df_average_by_category.loc[group_1][q1_name][0], df_average_by_category.loc[group_2][q1_name][0]\n",
    "    group_1, group_2 = group_1.item(), group_2.item()  # Actual values\n",
    "    \n",
    "    # Populate options\n",
    "    random_choice = random.choice([0, 1])  # Which option is correct\n",
    "    correct = 0 if ( mean_group_1 > mean_group_2 ) else 1\n",
    "    options = [ group_1, group_2 ]\n",
    "\n",
    "    # Generate question test\n",
    "    question_text = {\n",
    "        'text': 'Considering all data points for <b>{}</b>, which of the two following values of <b>{}</b> has the greater <u>average</u> <b>{}</b>?'.format(c_name, c_name, q1_name),\n",
    "        'structure': [\n",
    "            { 'type': 'text', 'value': 'Considering all data points for' },\n",
    "            { 'type': 'field', 'value': '{},'.format(c_name) },\n",
    "            { 'type': 'text', 'value': ', which of the two following values of' },\n",
    "            { 'type': 'field', 'value': c_name },\n",
    "            { 'type': 'text', 'value': 'has the greater' },\n",
    "            { 'type': 'operator', 'value': 'average' },\n",
    "            { 'type': 'field', 'value': q1_name }\n",
    "        ]  \n",
    "    }\n",
    "\n",
    "    # Return question\n",
    "    question = {\n",
    "        'q_field': q1_name,\n",
    "        'q1': q1_name,\n",
    "        'q2': q2_name,\n",
    "        'c': c_name,\n",
    "        'question': question_text,\n",
    "        'options': options,\n",
    "        'correct': correct,\n",
    "        'task': 'compare_values',\n",
    "        'annotated': []\n",
    "    }\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outlier_one_sided_MAD(points, thresh=3.5):\n",
    "    \"\"\"\n",
    "    From: https://stackoverflow.com/questions/22354094/pythonic-way-of-detecting-outliers-in-one-dimensional-observation-data\n",
    "    \n",
    "    Returns a boolean array with True if points are outliers and False \n",
    "    otherwise.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "        points : An numobservations by numdimensions array of observations\n",
    "        thresh : The modified z-score to use as a threshold. Observations with\n",
    "            a modified z-score (based on the median absolute deviation) greater\n",
    "            than this value will be classified as outliers.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        mask : A numobservations-length boolean array.\n",
    "\n",
    "    References:\n",
    "    ----------\n",
    "        Boris Iglewicz and David Hoaglin (1993), \"Volume 16: How to Detect and\n",
    "        Handle Outliers\", The ASQC Basic References in Quality Control:\n",
    "        Statistical Techniques, Edward F. Mykytka, Ph.D., Editor. \n",
    "    \"\"\"\n",
    "    if len(points.shape) == 1:\n",
    "        points = points[:,None]\n",
    "    median = np.median(points, axis=0)\n",
    "    diff = np.sum((points - median)**2, axis=-1)\n",
    "    diff = np.sqrt(diff)\n",
    "    med_abs_deviation = np.median(diff)\n",
    "\n",
    "    modified_z_score = 0.6745 * diff / med_abs_deviation\n",
    "\n",
    "    return modified_z_score > thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question_detect_outliers(df, abbreviation_to_field_name, dim=1):\n",
    "    c_name, q1_name, q2_name = abbreviation_to_field_name['c'], abbreviation_to_field_name['q1'], abbreviation_to_field_name['q2']\n",
    "\n",
    "    if dim == 1:\n",
    "        q_points = np.array(list(df[q1_name]))\n",
    "    elif dim == 2:\n",
    "        q_points = np.array(list(zip(df[q1_name], df[q2_name])))\n",
    "    outliers = detect_outlier_one_sided_MAD(q_points)\n",
    "    num_outliers = len([ o for o in outliers if o ])\n",
    "    percent_outliers = num_outliers / df.shape[0]\n",
    "    \n",
    "    # Populate options\n",
    "    options = [ 'Yes', 'No' ]\n",
    "    correct = 0 if num_outliers else 1\n",
    "    \n",
    "    # Generate question test\n",
    "    if dim == 1:\n",
    "        question_text = {\n",
    "            'text': 'Are there any outliers in <b>{}</b>?'.format(q1_name),\n",
    "            'structure': [\n",
    "                { 'type': 'text', 'value': 'Are there any outliers in' },\n",
    "                { 'type': 'field', 'value': q1_name }\n",
    "            ]  \n",
    "        }\n",
    "    elif dim == 2:\n",
    "        question_text = {\n",
    "            'text': 'Are there exceptions to the relationship between <b>{}</b> and <b>{}</b>?'.format(q1_name, q2_name),\n",
    "            'structure': [\n",
    "                { 'type': 'text', 'value': 'Are there exceptions to the relationship between' },\n",
    "                { 'type': 'field', 'value': q1_name },\n",
    "                { 'type': 'text', 'value': 'and' },\n",
    "                { 'type': 'field', 'value': q2_name }\n",
    "            ]  \n",
    "        }\n",
    "\n",
    "    # Return question\n",
    "    question = {\n",
    "        'q_field': q1_name,\n",
    "        'q1': q1_name,\n",
    "        'q2': q2_name,\n",
    "        'c': c_name,\n",
    "        'metadata': {\n",
    "          'percent_outliers': percent_outliers  \n",
    "        },\n",
    "        'question': question_text,\n",
    "        'options': options,\n",
    "        'correct': correct,\n",
    "        'task': 'compare_values',\n",
    "        'annotated': []\n",
    "    }\n",
    "    \n",
    "    return question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "corpuses = [ 'plotly', 'manyeyes', 'webtables', 'opendata' ]\n",
    "input_data_dir = '../randomly_selected_cqq_specs/'\n",
    "input_file_template = '{}_cqq_specs_with_data_all.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return { k: make_serializable(v) for (k, v) in obj.items() }\n",
    "    elif isinstance(obj, list) or isinstance(obj, tuple) or isinstance(obj, np.ndarray):\n",
    "        return [ make_serializable(d) for d in obj ]\n",
    "    elif isinstance(obj, (int, np.integer)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (float, np.float)):\n",
    "        return float(obj)\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(df, abbreviation_to_field_name, corpus, dataset_id, original_dataset_id, spec_id, task_type):\n",
    "    question = task_type_to_function[task_type](df, abbreviation_to_field_name)\n",
    "\n",
    "    if not question:\n",
    "        return None\n",
    "\n",
    "    question = make_serializable(question)\n",
    "    can_jsonify = json.loads(json.dumps(question))\n",
    "\n",
    "    # Augment\n",
    "    d = dict(question)\n",
    "    d['cardinality'] = len(list(df[abbreviation_to_field_name['c']].unique()))\n",
    "    d['corpus'] = corpus\n",
    "    d['dataset_id'] = '{}.json'.format(dataset_id)\n",
    "    d['original_dataset_id'] = original_dataset_id\n",
    "    d['spec_id'] = spec_id\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_per_source_dataset = 10\n",
    "\n",
    "def get_sample_from_path(path, sample=3000, row_count=None, header=True, random_sample=True, columns=[]):\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        sep='\\t',\n",
    "        nrows=10000000\n",
    "    )\n",
    "    df.columns = columns\n",
    "    df.drop_duplicates(subset=['dataset_id', 'combination_number'], inplace=True)  # Drop dups\n",
    "    df = df.groupby('dataset_id').head(max_per_source_dataset).reset_index(drop=True)   # Cap number per dataset\n",
    "    df = df.sample(frac=1).reset_index(drop=True)  # Randomize\n",
    "    # count_per_dataset_id = dict(df.groupby('dataset_id').count())\n",
    "    # Max out number of datasets per source dataset\n",
    "    \n",
    "#    dataset_ids = np.unique(list(df['dataset_id']))   \n",
    "    count_per_dataset_id = dict(df.groupby('dataset_id').count())\n",
    "    # print(df.head())\n",
    "    df = df.iloc[:sample, :]\n",
    "    print(df.shape)\n",
    "    return df\n",
    "\n",
    "# Container for sampled datasets\n",
    "sample_datasets_all_corpuses = pd.DataFrame(columns=['corpus', 'locator', 'dataset_id', 'combination_number', 'column_metadata', 'data'])\n",
    "for corpus in corpuses:\n",
    "    print('Corpus:', corpus)\n",
    "    input_file_name = input_file_template.format(corpus)\n",
    "    input_file_path = join(input_data_dir, input_file_name)\n",
    "    #corpus_datasets_df = get_sample_from_path(input_file_path, sample=100000, columns=['corpus', 'locator', 'dataset_id', 'combination_number', 'column_metadata', 'data'])\n",
    "    corpus_datasets_df = get_sample_from_path(input_file_path, sample=6000, columns=['corpus', 'locator', 'dataset_id', 'combination_number', 'column_metadata', 'data'])\n",
    "    \n",
    "    corpus_datasets_df = corpus_datasets_df.sample(frac=1).reset_index(drop=True)\n",
    "    sample_datasets_all_corpuses = sample_datasets_all_corpuses.append(corpus_datasets_df, ignore_index=True)\n",
    "    \n",
    "sample_datasets_all_corpuses = sample_datasets_all_corpuses.sample(frac=1).reset_index(drop=True)\n",
    "print(sample_datasets_all_corpuses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_column_and_group_names(df, abbreviation_to_field_name):\n",
    "    df.rename(index=str, columns={\n",
    "        abbreviation_to_field_name['c']: 'C',\n",
    "        abbreviation_to_field_name['q1']: 'W',\n",
    "        abbreviation_to_field_name['q2']: 'Z',\n",
    "    }, inplace=True)\n",
    "  \n",
    "    replacement_dict = {}\n",
    "    unique_groups = np.unique(df['C'])\n",
    "    for i, g in enumerate(unique_groups):\n",
    "        replacement_dict[g] = 'Group {}'.format(i)\n",
    "    df.replace(replacement_dict, inplace=True)\n",
    "    \n",
    "    abbreviation_to_field_name['c'] = 'C'\n",
    "    abbreviation_to_field_name['q1'] = 'W'\n",
    "    abbreviation_to_field_name['q2'] = 'Z'\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df, abbreviation_to_field_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type_to_function = {\n",
    "    'read_value': generate_question_find_value,\n",
    "    'compare_values': generate_question_compare_values,\n",
    "    'find_maximum': generate_question_find_maximum,\n",
    "    'compare_averages': generate_question_compare_averages,\n",
    "    'detect_outliers': generate_question_detect_outliers\n",
    "}\n",
    "task_types = [ k for k in task_type_to_function.keys() ]\n",
    "\n",
    "data_dir = join(output_dir, 'data')\n",
    "shutil.rmtree(data_dir, ignore_errors=True)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Preserve datasets for later feature_extraction\n",
    "valid_datasets = pd.DataFrame(columns=['corpus', 'locator', 'dataset_id', 'combination_number', 'column_metadata', 'data'])\n",
    "\n",
    "total = 0\n",
    "valid = 0\n",
    "valid_per_corpus = Counter()\n",
    "\n",
    "limit_per_corpus = 3000\n",
    "print_every = 100\n",
    "task_to_questions = { k: [] for k in task_type_to_function.keys() }\n",
    "character_limit = 200\n",
    "num_tasks = len(task_types)\n",
    "num_specs = len(raw_specs)\n",
    "spec_ids = [ v['description']['id'] for v in raw_specs ]\n",
    "max_per_dataset_id = 10\n",
    "\n",
    "# Deterministically populate v, t\n",
    "num_per_v_t = 250 \n",
    "expected_number_per_corpus_per_v_t = num_per_v_t / len(corpuses)\n",
    "count_per_v_t = { s: { t: 0 for t in range(0, num_specs) } for s in spec_ids }\n",
    "per_corpus_per_v_t = Counter()\n",
    "current_spec_id = 0\n",
    "current_task_id = 0\n",
    "\n",
    "for (i, row) in sample_datasets_all_corpuses.iterrows():\n",
    "    corpus = row['corpus']\n",
    "    if per_corpus_per_v_t[corpus] >= expected_number_per_corpus_per_v_t: continue\n",
    "    total += 1\n",
    "\n",
    "    original_dataset_id = row['dataset_id']\n",
    "    combination_number = row['combination_number']\n",
    "    safe_dataset_id = slugify(original_dataset_id)\n",
    "    safe_dataset_id = safe_dataset_id[:min(len(safe_dataset_id), character_limit)]\n",
    "    dataset_id = '{}__{}'.format(safe_dataset_id, combination_number)\n",
    "    column_metadata = json.loads(row['column_metadata'])\n",
    "    \n",
    "    try:\n",
    "        data = row['data']\n",
    "        df = pd.read_json(\n",
    "            data,\n",
    "            dtype={ cm['name']: np.dtype(cm['dtype']) for cm in column_metadata }\n",
    "        )\n",
    "        df = df[[ cm['name'] for cm in column_metadata ]]\n",
    "    except Exception as e:\n",
    "        print('Error reading data')\n",
    "        continue\n",
    "    \n",
    "    abbreviation_to_field_name = {\n",
    "        'c': [ cm['name'] for cm in column_metadata if (cm['general_type'] == 'c')][0],\n",
    "        'q1': [ cm['name'] for cm in column_metadata if (cm['general_type'] == 'q')][0],\n",
    "        'q2': [ cm['name'] for cm in column_metadata if (cm['general_type'] == 'q')][1]\n",
    "    }\n",
    "        \n",
    "    df, abbreviation_to_field_name = format_column_and_group_names(df, abbreviation_to_field_name)\n",
    "    spec = raw_specs[current_spec_id]\n",
    "    spec_id = spec['description']['id']\n",
    "    task_type = task_types[current_task_id]\n",
    "\n",
    "    try:\n",
    "        question = generate_question(df, abbreviation_to_field_name, corpus, dataset_id, original_dataset_id, spec_id, task_type)\n",
    "    except Exception as e:\n",
    "        print('Error generating question for task', task_type, e)\n",
    "        continue\n",
    "    try:\n",
    "        fields_in_question = [question['c'], question['q1'], question['q2']]\n",
    "        fields_in_df = list(df.columns)\n",
    "        if (set(fields_in_question) != set(fields_in_df)):\n",
    "            print('Mismatched fields', fields_in_df, fields_in_question)\n",
    "    except Exception as e:\n",
    "        print('Error matching fields', e)\n",
    "        continue\n",
    "\n",
    "    per_corpus_per_v_t[corpus] += 1\n",
    "\n",
    "    valid += 1\n",
    "    valid_per_corpus[corpus] += 1\n",
    "\n",
    "    task_to_questions[task_type].append(question)\n",
    "    valid_datasets = valid_datasets.append({\n",
    "        'corpus': corpus,\n",
    "        'locator': row['locator'],\n",
    "        'dataset_id': dataset_id,\n",
    "        'combination_number': combination_number,\n",
    "        'column_metadata': column_metadata,\n",
    "        'data': df.to_json(orient='records')  \n",
    "    }, ignore_index=True)  # Save row for later\n",
    "\n",
    "    if total % print_every == 0: print('[{}/{}] Corpus: {}, Dataset ID: {}'.format(valid, total, corpus, dataset_id))\n",
    "\n",
    "    # Copy over dataset file\n",
    "    output_file_name = join(data_dir, '{}.json'.format(dataset_id))\n",
    "    if os.path.isfile(output_file_name):\n",
    "        print('File already exists')\n",
    "        print(corpus, combination_number, dataset_id)\n",
    "    else:\n",
    "        df.to_json(\n",
    "            output_file_name,\n",
    "            orient='records'\n",
    "        )\n",
    "        j = df.to_json(\n",
    "            orient='records'\n",
    "        )\n",
    "        print(j)\n",
    "\n",
    "    count_per_v_t[current_spec_id][current_task_id] += 1\n",
    "    if count_per_v_t[current_spec_id][current_task_id] >= num_per_v_t:\n",
    "        print('Spec ID:', current_spec_id)\n",
    "        print('Task ID:', current_task_id)\n",
    "        per_corpus_per_v_t = Counter()\n",
    "        if current_task_id < len(task_types) - 1:  # Increment task\n",
    "            current_task_id += 1\n",
    "        else:\n",
    "            if current_spec_id < len(raw_specs) - 1:\n",
    "                current_spec_id += 1  # Increment spec\n",
    "                current_task_id = 0\n",
    "            else:\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
